#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•çˆ¬è™«æºç®¡ç†å™¨åŠŸèƒ½
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

# å¯¼å…¥çˆ¬è™«æºç®¡ç†å™¨
from app.services.crawler_source_manager import crawler_source_manager

def test_get_all_configs():
    """æµ‹è¯•è·å–æ‰€æœ‰çˆ¬è™«é…ç½®"""
    print("=== æµ‹è¯•è·å–æ‰€æœ‰çˆ¬è™«é…ç½® ===")
    configs = crawler_source_manager.get_all_crawler_configs(refresh=True)
    print(f"âœ… è·å–åˆ° {len(configs)} ä¸ªçˆ¬è™«é…ç½®")
    
    for config_id, config in configs.items():
        print(f"\né…ç½®ID: {config_id}")
        print(f"åç§°: {config.get('name')}")
        print(f"URL: {config.get('url')}")
        print(f"æ•°æ®æºç±»å‹: {config.get('source_type')}")
        print(f"å¯ç”¨çŠ¶æ€: {config.get('enabled')}")
        
    return configs

def test_run_crawler_by_config(config_id):
    """æµ‹è¯•é€šè¿‡é…ç½®IDè¿è¡Œçˆ¬è™«"""
    print(f"\n=== æµ‹è¯•é€šè¿‡é…ç½®IDè¿è¡Œçˆ¬è™« (ID: {config_id}) ===")
    
    # è¿è¡Œçˆ¬è™«ï¼Œè®¾ç½®limit=5
    result = crawler_source_manager.run_crawler_by_config(
        config_id=config_id,
        params={"limit": 5}
    )
    
    print(f"âœ… è¿è¡Œç»“æœ: {'æˆåŠŸ' if result.get('success') else 'å¤±è´¥'}")
    
    if result.get('success'):
        print(f"ğŸ“‹ ç»“æœæ•°é‡: {result.get('total_results', 0)}")
        print(f"ğŸ“„ æ¶ˆæ¯: {result.get('message')}")
        
        # æ‰“å°å‰3æ¡ç»“æœ
        results = result.get('results', [])
        if results:
            print("\n=== éƒ¨åˆ†ç»“æœ ===")
            for i, item in enumerate(results[:3], 1):
                print(f"\nç»“æœ {i}:")
                print(f"æ ‡é¢˜: {item.get('title', 'æ— ')}")
                print(f"é“¾æ¥: {item.get('url', 'æ— ')}")
                if 'content' in item:
                    print(f"å†…å®¹: {item['content'][:100]}...")
    else:
        print(f"âŒ é”™è¯¯ä»£ç : {result.get('error_code')}")
        print(f"âŒ é”™è¯¯ä¿¡æ¯: {result.get('error_message')}")
    
    return result

def test_run_crawler_by_source(source_name):
    """æµ‹è¯•é€šè¿‡æ•°æ®æºç±»å‹è¿è¡Œçˆ¬è™«"""
    print(f"\n=== æµ‹è¯•é€šè¿‡æ•°æ®æºç±»å‹è¿è¡Œçˆ¬è™« (ç±»å‹: {source_name}) ===")
    
    # è¿è¡Œçˆ¬è™«ï¼Œè®¾ç½®limit=5
    result = crawler_source_manager.run_crawler_by_source(
        source_name=source_name,
        params={"keyword": "Python çˆ¬è™«", "limit": 5}
    )
    
    print(f"âœ… è¿è¡Œç»“æœ: {'æˆåŠŸ' if result.get('success') else 'å¤±è´¥'}")
    
    if result.get('success'):
        print(f"ğŸ“‹ ç»“æœæ•°é‡: {result.get('total_results', 0)}")
        print(f"ğŸ“„ æ¶ˆæ¯: {result.get('message')}")
        
        # æ‰“å°å‰3æ¡ç»“æœ
        results = result.get('results', [])
        if results:
            print("\n=== éƒ¨åˆ†ç»“æœ ===")
            for i, item in enumerate(results[:3], 1):
                print(f"\nç»“æœ {i}:")
                print(f"æ ‡é¢˜: {item.get('title', 'æ— ')}")
                print(f"é“¾æ¥: {item.get('url', 'æ— ')}")
                if 'abstract' in item:
                    print(f"æ‘˜è¦: {item['abstract'][:100]}...")
    else:
        print(f"âŒ é”™è¯¯ä»£ç : {result.get('error_code')}")
        print(f"âŒ é”™è¯¯ä¿¡æ¯: {result.get('error_message')}")
    
    return result

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=== å¼€å§‹æµ‹è¯•çˆ¬è™«æºç®¡ç†å™¨ ===")
    
    # æµ‹è¯•è·å–æ‰€æœ‰é…ç½®
    configs = test_get_all_configs()
    
    # å¦‚æœæœ‰é…ç½®ï¼Œæµ‹è¯•è¿è¡Œç¬¬ä¸€ä¸ªé…ç½®
    if configs:
        first_config_id = list(configs.keys())[0]
        test_run_crawler_by_config(first_config_id)
    
    # æµ‹è¯•é€šè¿‡æ•°æ®æºç±»å‹è¿è¡Œçˆ¬è™«
    test_run_crawler_by_source("baidu_search")
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")

if __name__ == "__main__":
    main()
